{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# English to French Translation using GPT-2\n", "This notebook fine-tunes GPT-2 for English-to-French translation and runs inference with Hugging Face `pipeline`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install dependencies\n", "!pip install transformers datasets accelerate -q"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from datasets import load_dataset\n", "\n", "# Load a parallel dataset (English-French)\n", "dataset = load_dataset('opus_books', 'en-fr')\n", "dataset = dataset['train'].train_test_split(test_size=0.1)\n", "\n", "print(dataset)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import AutoTokenizer\n", "\n", "model_checkpoint = 'gpt2'\n", "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n", "\n", "# Add padding token if missing\n", "if tokenizer.pad_token is None:\n", "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n", "\n", "def preprocess(examples):\n", "    inputs = [ex for ex in examples['translation']]\n", "    en = [x['en'] for x in inputs]\n", "    fr = [x['fr'] for x in inputs]\n", "    # Format as: \"Translate English to French: <en> => <fr>\"\n", "    inputs_with_targets = [f\"Translate English to French: {e} => {f}\" for e, f in zip(en, fr)]\n", "    return tokenizer(inputs_with_targets, truncation=True, padding='max_length', max_length=128)\n", "\n", "tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=dataset['train'].column_names)\n", "tokenized_dataset.set_format('torch')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n", "\n", "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n", "model.resize_token_embeddings(len(tokenizer))\n", "\n", "training_args = TrainingArguments(\n", "    output_dir='./results',\n", "    evaluation_strategy='epoch',\n", "    save_strategy='epoch',\n", "    learning_rate=5e-5,\n", "    per_device_train_batch_size=4,\n", "    per_device_eval_batch_size=4,\n", "    num_train_epochs=1,  # For demo; increase for better results\n", "    weight_decay=0.01,\n", "    push_to_hub=False,\n", "    logging_dir='./logs',\n", ")\n", "\n", "trainer = Trainer(\n", "    model=model,\n", "    args=training_args,\n", "    train_dataset=tokenized_dataset['train'],\n", "    eval_dataset=tokenized_dataset['test'],\n", "    tokenizer=tokenizer,\n", ")\n", "\n", "trainer.train()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Save model\n", "trainer.save_model('./gpt2-translation')\n", "tokenizer.save_pretrained('./gpt2-translation')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import pipeline\n", "\n", "translator = pipeline('text-generation', model='./gpt2-translation', tokenizer='./gpt2-translation')\n", "\n", "def translate(text):\n", "    prompt = f'Translate English to French: {text} =>'\n", "    result = translator(prompt, max_length=60, num_return_sequences=1)\n", "    return result[0]['generated_text']\n", "\n", "print(translate('Hello, how are you?'))\n", "print(translate('I love learning new languages.'))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.9"}}, "nbformat": 4, "nbformat_minor": 5}