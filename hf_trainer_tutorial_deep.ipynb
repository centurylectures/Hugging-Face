{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52708852",
   "metadata": {},
   "source": [
    "# Hugging Face Trainer — Deep Dive Tutorial\n",
    "\n",
    "*A comprehensive, code‑along notebook with in‑depth explanations before each code cell.*\n",
    "\n",
    "> **Last updated:** 2025-09-17 09:17 UTC\n",
    "> \n",
    "> **What you’ll learn**\n",
    "> - Conceptual background of the Trainer API\n",
    "> - Preparing datasets and tokenization\n",
    "> - Loading pre‑trained models for classification\n",
    "> - Deep dive into `TrainingArguments`\n",
    "> - What happens inside the training loop\n",
    "> - Evaluation, prediction, and metrics\n",
    "> - Saving and resuming checkpoints\n",
    "> - Sharing models via the Hugging Face Hub\n",
    "> - Advanced usage: custom collators and callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33eb68c",
   "metadata": {},
   "source": [
    "## 0) Prerequisites & Environment\n",
    "\n",
    "The Hugging Face **Trainer API** is a high‑level abstraction built on top of PyTorch (and optionally TensorFlow).  \n",
    "It automates most of the boilerplate needed for training large language models while still giving you hooks for customization.\n",
    "\n",
    "**What the Trainer does for you:**\n",
    "- Handles the full training loop (forward pass, loss computation, backpropagation, optimizer step).\n",
    "- Manages evaluation at defined intervals.\n",
    "- Saves and loads checkpoints (including optimizer/scheduler states).\n",
    "- Supports distributed and mixed‑precision training out of the box.\n",
    "- Logs metrics for you (to stdout, TensorBoard, WandB, etc.).\n",
    "\n",
    "You’ll need:\n",
    "- `transformers` (for Trainer, models, tokenizers)\n",
    "- `datasets` (to load IMDB dataset)\n",
    "- `evaluate` (for accuracy metric)\n",
    "- `torch` (deep learning backend)\n",
    "\n",
    "> If you don’t have them installed, run the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Install dependencies\n",
    "# %pip install -U transformers datasets evaluate torch torchvision torchaudio accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968eaef8",
   "metadata": {},
   "source": [
    "## 1) Load & Prepare a Dataset\n",
    "\n",
    "We’ll use the **IMDB dataset** for binary sentiment classification (positive vs. negative).\n",
    "\n",
    "### Key ideas\n",
    "- `datasets.load_dataset` automatically downloads and caches datasets from the Hugging Face Hub.\n",
    "- Each split (`train`, `test`) is a `Dataset` object containing rows of data.\n",
    "- We must **tokenize** raw text into model‑readable IDs before feeding it into a Transformer.\n",
    "\n",
    "### Tokenization details\n",
    "- `AutoTokenizer` chooses the right tokenizer for the model architecture (e.g., WordPiece for BERT).\n",
    "- We enable `truncation=True` so reviews longer than the model’s max length (512 for BERT‑like models) are truncated.\n",
    "- Padding can be applied dynamically later (better for efficiency).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f29f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83aa75c",
   "metadata": {},
   "source": [
    "## 2) Load a Pre‑Trained Model\n",
    "\n",
    "We use **DistilBERT**, a smaller, faster variant of BERT.  \n",
    "For classification, we need a **classification head** on top of the transformer encoder.\n",
    "\n",
    "- `AutoModelForSequenceClassification` automatically attaches the right head.\n",
    "- `num_labels=2` indicates binary classification (positive vs. negative).\n",
    "\n",
    "> Under the hood, the model outputs **logits** (unnormalized scores), which are then passed to softmax for probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c20933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f622cedb",
   "metadata": {},
   "source": [
    "## 3) Define TrainingArguments\n",
    "\n",
    "The **TrainingArguments** class is the backbone of training configuration.  \n",
    "It controls **when, how, and where** training happens.\n",
    "\n",
    "### Under the hood\n",
    "When passed to `Trainer`, these arguments:\n",
    "- Configure the optimizer (`AdamW` by default) and learning rate scheduler.\n",
    "- Define batch sizes, number of epochs, and gradient accumulation.\n",
    "- Decide when evaluation and checkpoint saving occurs.\n",
    "- Control device placement (CPU, GPU, TPU) and mixed precision (fp16/bf16).\n",
    "\n",
    "### Important parameters\n",
    "- `output_dir`: where checkpoints and logs are written.\n",
    "- `evaluation_strategy`: when to run evaluation (`\"no\"`, `\"steps\"`, or `\"epoch\"`).\n",
    "- `save_strategy`: how often to save checkpoints.\n",
    "- `per_device_train_batch_size`: batch size per GPU/CPU device.\n",
    "- `num_train_epochs`: how many epochs to train for.\n",
    "- `learning_rate`: optimizer learning rate.\n",
    "- `weight_decay`: regularization to prevent overfitting.\n",
    "- `logging_steps`: how often to log training progress.\n",
    "- `push_to_hub`: whether to upload checkpoints automatically.\n",
    "\n",
    "### Best practices\n",
    "- Use small batch sizes if limited by GPU memory and scale up gradually.\n",
    "- Combine `gradient_accumulation_steps` with small batch sizes to simulate larger ones.\n",
    "- Always specify `seed` for reproducibility.\n",
    "- Use `load_best_model_at_end=True` with `metric_for_best_model` for automatic model selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d2ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/mnt/data/imdb_trainer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"/mnt/data/logs\",\n",
    "    logging_steps=100,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7338596e",
   "metadata": {},
   "source": [
    "## 4) Define Evaluation Metrics\n",
    "\n",
    "Trainer does not hardcode metrics. Instead, you provide a function (`compute_metrics`) that computes them from predictions.\n",
    "\n",
    "### How it works\n",
    "- During evaluation, the Trainer gets raw logits and labels.\n",
    "- It calls `compute_metrics(eval_pred)` with a tuple `(logits, labels)`.\n",
    "- You return a dictionary of metrics to be logged and displayed.\n",
    "\n",
    "We’ll use the `evaluate` library to compute **accuracy**.\n",
    "\n",
    "> You can also compute F1, precision, recall, BLEU, ROUGE, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac000bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025a3ff2",
   "metadata": {},
   "source": [
    "## 5) Create the Trainer\n",
    "\n",
    "The `Trainer` object brings everything together:\n",
    "- The model (with classification head).\n",
    "- The dataset (train/eval splits).\n",
    "- The tokenizer (needed for saving/pushing to Hub).\n",
    "- The training arguments.\n",
    "- The metric function.\n",
    "\n",
    "Under the hood, `.train()` will:\n",
    "1. Shuffle and batch the dataset.\n",
    "2. Run forward passes and compute loss.\n",
    "3. Backpropagate gradients and update weights.\n",
    "4. Evaluate at specified intervals.\n",
    "5. Save checkpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e607d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"].shuffle(seed=42).select(range(2000)),\n",
    "    eval_dataset=tokenized[\"test\"].shuffle(seed=42).select(range(1000)),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f216cdb",
   "metadata": {},
   "source": [
    "## 6) Train the Model\n",
    "\n",
    "Calling `.train()` launches the full training loop.\n",
    "\n",
    "- Logs training loss periodically.\n",
    "- Runs evaluation at the end of each epoch (per `evaluation_strategy`).\n",
    "- Saves checkpoints into `output_dir`.\n",
    "- Supports resuming if training is interrupted.\n",
    "\n",
    "> Checkpoints contain: model weights, optimizer state, scheduler state, RNG state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a701ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542752dd",
   "metadata": {},
   "source": [
    "## 7) Evaluate & Predict\n",
    "\n",
    "After training, you’ll want to check performance and make predictions.\n",
    "\n",
    "- `.evaluate()` runs the evaluation loop on a dataset and returns metrics.\n",
    "- `.predict()` returns raw logits, true labels, and computed metrics if available.\n",
    "\n",
    "### Note\n",
    "The logits must be converted to predictions (e.g., `argmax` for classification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be49d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "preds = trainer.predict(tokenized[\"test\"].select(range(10)))\n",
    "preds.predictions.argmax(axis=-1), preds.label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b7006",
   "metadata": {},
   "source": [
    "## 8) Save, Load, and Resume Checkpoints\n",
    "\n",
    "Trainer automatically saves checkpoints, but you can also call `.save_model()`.\n",
    "\n",
    "- **Manual save**: writes model weights and tokenizer files.\n",
    "- **Resume**: `trainer.train(resume_from_checkpoint=True)` continues training from the last checkpoint.\n",
    "\n",
    "> This ensures experiments are reproducible and training can survive interruptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839520a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/mnt/data/imdb_trained_model\")\n",
    "\n",
    "# Reloading\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "reloaded = AutoModelForSequenceClassification.from_pretrained(\"/mnt/data/imdb_trained_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8515c738",
   "metadata": {},
   "source": [
    "## 9) Push to the Hub\n",
    "\n",
    "The Hugging Face Hub is the central place to share models.\n",
    "\n",
    "### How it works\n",
    "1. Authenticate with `huggingface-cli login` or `huggingface_hub.login()`.\n",
    "2. Set `push_to_hub=True` in `TrainingArguments` **or** call `trainer.push_to_hub()`.\n",
    "3. A new repo is created under your account if it doesn’t exist.\n",
    "4. Each push creates a new commit with model weights and config.\n",
    "\n",
    "> Include a **Model Card** (README.md) to document dataset, metrics, and intended use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19daa0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub(\"my-imdb-distilbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d9d8b",
   "metadata": {},
   "source": [
    "## 10) Advanced: Custom Data Collator\n",
    "\n",
    "A **data collator** handles how batches are formed.  \n",
    "The default simply stacks features, but for NLP tasks we often need **dynamic padding**.\n",
    "\n",
    "- `DataCollatorWithPadding` pads each batch to the length of the longest sequence in that batch.\n",
    "- This is more memory efficient than static padding to a fixed length.\n",
    "\n",
    "You can also write a fully custom collator (e.g., for multi‑task learning).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396dc66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"].shuffle(seed=42).select(range(2000)),\n",
    "    eval_dataset=tokenized[\"test\"].shuffle(seed=42).select(range(1000)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d875bc22",
   "metadata": {},
   "source": [
    "## 11) Advanced: Custom Callbacks\n",
    "\n",
    "`TrainerCallback` lets you hook into training events.  \n",
    "Useful for:\n",
    "- Early stopping\n",
    "- Custom logging\n",
    "- Learning rate adjustments\n",
    "- Notifications (e.g., Slack/Discord alerts)\n",
    "\n",
    "Callbacks are called at events like: `on_train_begin`, `on_epoch_end`, `on_log`, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67248b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        print(\"Log:\", logs)\n",
    "\n",
    "trainer.add_callback(PrinterCallback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31106726",
   "metadata": {},
   "source": [
    "## 12) Wrap‑Up & Next Steps\n",
    "\n",
    "You’ve now seen the **Trainer API** in action, with both basic and advanced features.\n",
    "\n",
    "### Key takeaways\n",
    "- **Trainer** abstracts the boilerplate but is highly configurable.\n",
    "- **TrainingArguments** is the central config object — learn its parameters well.\n",
    "- Use `compute_metrics` to evaluate meaningfully.\n",
    "- Save/resume checkpoints to make experiments reproducible.\n",
    "- Push to the Hub to share and collaborate.\n",
    "\n",
    "### Where to go next\n",
    "- Try multi‑GPU training with `accelerate` integration.\n",
    "- Explore mixed‑precision training (`fp16`/`bf16`) for faster runs.\n",
    "- Experiment with custom schedulers and optimizers.\n",
    "- Build advanced callbacks (early stopping, metric‑based LR scheduling).\n",
    "\n",
    "**Resources:**\n",
    "- [Trainer documentation](https://huggingface.co/docs/transformers/main/en/main_classes/trainer)\n",
    "- [TrainingArguments reference](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "- [Transformers examples](https://github.com/huggingface/transformers/tree/main/examples)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
