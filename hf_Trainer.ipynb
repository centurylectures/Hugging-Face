{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f7762e3",
      "metadata": {
        "id": "8f7762e3"
      },
      "source": [
        "# ü§ó Hugging Face Trainer - Detailed Training Lab\n",
        "This notebook is a comprehensive guide to using Hugging Face's `Trainer` API for training and fine-tuning transformer models. It provides detailed explanations of each component, including dataset loading, tokenization, model selection, training argument configuration, custom optimizer integration, metric evaluation, and model saving.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6496a08c",
      "metadata": {
        "id": "6496a08c"
      },
      "source": [
        "## üìã Overview\n",
        "We will walk through the following steps:\n",
        "1. **Load and explore a dataset**\n",
        "2. **Tokenize and preprocess the data**\n",
        "3. **Load a pre-trained model for classification**\n",
        "4. **Configure training arguments**\n",
        "5. **Customize the optimizer and learning rate scheduler**\n",
        "6. **Define the Hugging Face `Trainer`**\n",
        "7. **Train and evaluate the model**\n",
        "8. **Save the fine-tuned model**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cedd370",
      "metadata": {
        "id": "0cedd370"
      },
      "source": [
        "## üì¶ Install Required Libraries\n",
        "Install Hugging Face Transformers, Datasets, and Evaluate packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0f235bae",
      "metadata": {
        "id": "0f235bae"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers datasets evaluate -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd32b240",
      "metadata": {
        "id": "cd32b240"
      },
      "source": [
        "## üìö Import Libraries\n",
        "We import all necessary components from the Transformers and Datasets libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "203500e6",
      "metadata": {
        "id": "203500e6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import  get_scheduler\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ[\"WANDB_MODE\"]=\"offline\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee158630",
      "metadata": {
        "id": "ee158630"
      },
      "source": [
        "## üóÇÔ∏è Load and Explore the Dataset\n",
        "We use the IMDb dataset, which is a binary sentiment classification dataset (positive/negative reviews)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "793796e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "793796e4",
        "outputId": "a5cefb7c-b2cd-42cc-94ab-5ecea0ff7c5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier\\'s plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it\\'s the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...',\n",
              " 'label': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e0d3510",
      "metadata": {
        "id": "7e0d3510"
      },
      "source": [
        "## ‚úÇÔ∏è Tokenize the Dataset\n",
        "We use a tokenizer corresponding to a pre-trained transformer model to tokenize the raw text data. Tokenization is essential to convert text into input IDs and attention masks suitable for transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2b0ccdf7",
      "metadata": {
        "id": "2b0ccdf7"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d66f3a96",
      "metadata": {
        "id": "d66f3a96"
      },
      "source": [
        "## üß† Load Pre-trained Model\n",
        "We load a pre-trained DistilBERT model for sequence classification. The model head is adjusted for binary classification (2 labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "68d41387",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68d41387",
        "outputId": "fa153ea4-8b82-4e4d-f08e-4b0e90849c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cafd02e7",
      "metadata": {
        "id": "cafd02e7"
      },
      "source": [
        "## ‚öôÔ∏è Configure Training Arguments\n",
        "`TrainingArguments` is a configuration class to customize the training process. You can set batch sizes, learning rate, evaluation strategy, logging, weight decay, and other parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a25d1c6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a25d1c6b",
        "outputId": "333e1306-b347-49fc-c1bb-0b1558b5a36b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    report_to=None, # \"wandb\"\n",
        "    output_dir=\"./results\",                 # Where to store model checkpoints\n",
        "    eval_strategy=\"epoch\",            # Evaluate after every epoch\n",
        "    save_strategy=\"epoch\",                  # Save model after every epoch\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,                        # L2 regularization\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True              # Load best checkpoint (based on eval metric)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d928206",
      "metadata": {
        "id": "0d928206"
      },
      "source": [
        "## üõ† Custom Optimizer and Learning Rate Scheduler\n",
        "Instead of using the default optimizer/scheduler, we define our own:\n",
        "- `AdamW` is a popular optimizer for transformers\n",
        "- `get_scheduler` allows for linear decay of the learning rate during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1b1edf19",
      "metadata": {
        "id": "1b1edf19"
      },
      "outputs": [],
      "source": [
        "# Define custom optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate, weight_decay=training_args.weight_decay)\n",
        "\n",
        "# Setup learning rate scheduler\n",
        "num_training_steps = len(tokenized_datasets['train']) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d45efb9e",
      "metadata": {
        "id": "d45efb9e"
      },
      "source": [
        "## üìè Define Evaluation Metrics\n",
        "We use `accuracy` as the evaluation metric. The `compute_metrics` function will be called by the `Trainer` during evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "104f82ed",
      "metadata": {
        "id": "104f82ed"
      },
      "outputs": [],
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "347e313e",
      "metadata": {
        "id": "347e313e"
      },
      "source": [
        "## üß™ Define the Trainer\n",
        "The `Trainer` class handles the training loop, evaluation, and saving. You pass in the model, datasets, tokenizer, training arguments, metrics function, and optimizer/scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0d03a4e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d03a4e7",
        "outputId": "2920a5be-bc5c-4ab0-b075-83a75ad7129d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1018650385.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000)),\n",
        "    eval_dataset=tokenized_datasets[\"test\"].select(range(1000)),\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    optimizers=(optimizer, lr_scheduler)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15e4d5b7",
      "metadata": {
        "id": "15e4d5b7"
      },
      "source": [
        "## üöÄ Train the Model\n",
        "We now train the model using the `train()` method of the `Trainer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f2e24fa0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "f2e24fa0",
        "outputId": "ec168d68-09fd-4d40-da2c-31a1eb181f50"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 05:51, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.246400</td>\n",
              "      <td>0.287871</td>\n",
              "      <td>0.884000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.221700</td>\n",
              "      <td>0.368066</td>\n",
              "      <td>0.885000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.435541</td>\n",
              "      <td>0.878000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=375, training_loss=0.24622172705332437, metrics={'train_runtime': 353.0413, 'train_samples_per_second': 16.995, 'train_steps_per_second': 1.062, 'total_flos': 790006588340928.0, 'train_loss': 0.24622172705332437, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4e410a1",
      "metadata": {
        "id": "e4e410a1"
      },
      "source": [
        "## üìä Evaluate the Model\n",
        "Use the `evaluate()` method to get performance metrics on the evaluation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "933ea8a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "933ea8a1",
        "outputId": "cedfe7b6-defe-412f-9011-639ea2e68566"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:15]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.28787073493003845,\n",
              " 'eval_accuracy': 0.884,\n",
              " 'eval_runtime': 15.7478,\n",
              " 'eval_samples_per_second': 63.501,\n",
              " 'eval_steps_per_second': 4.001,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a71d7a5",
      "metadata": {
        "id": "7a71d7a5"
      },
      "source": [
        "## üíæ Save the Fine-tuned Model\n",
        "After training, save the model to disk for later use or deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1798afd8",
      "metadata": {
        "id": "1798afd8"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"./fine-tuned-imdb\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}