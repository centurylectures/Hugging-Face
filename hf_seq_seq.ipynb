{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to French Translation with Hugging Face\n",
    "\n",
    "This notebook demonstrates how to fine-tune a pre-trained `Helsinki-NLP/opus-mt-en-fr` model for English-to-French translation using the Hugging Face `transformers`, `datasets`, and `accelerate` libraries.\n",
    "\n",
    "We will cover the following steps:\n",
    "1.  **Setup**: Install and import the necessary libraries.\n",
    "2.  **Load Data**: Load a sample dataset for translation.\n",
    "3.  **Preprocessing**: Tokenize the source (English) and target (French) texts.\n",
    "4.  **Fine-Tuning**: Set up the trainer and fine-tune the model on our dataset.\n",
    "5.  **Inference**: Use the fine-tuned model with the `pipeline` API to translate new sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install the required libraries. We need `transformers` for the models, `datasets` to handle the data, `accelerate` to optimize training, and `sacrebleu` for evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[torch] datasets sacrebleu accelerate -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import all the necessary components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "We'll use the `opus_books` dataset, which contains translated texts from books. We will use the English-French (`en-fr`) pair. To make the training faster for this demonstration, we'll only use a small portion of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a smaller subset for demonstration purposes\n",
    "raw_dataset = load_dataset(\"opus_books\", \"en-fr\", split='train[:1%]')\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "split_dataset = raw_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "print(\"Training set size:\", len(split_dataset['train']))\n",
    "print(\"Validation set size:\", len(split_dataset['test']))\n",
    "print(\"\\nSample:\", split_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "Next, we need to convert our text data into a format the model can understand. We'll use a tokenizer that corresponds to our pre-trained model.\n",
    "\n",
    "The `Helsinki-NLP/opus-mt-en-fr` model is a great choice for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a preprocessing function to tokenize the English text as input and the French text as the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # examples['translation'] is a list of dicts {'en': ..., 'fr': ...}\n",
    "    inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    \n",
    "    # Tokenize targets\n",
    "    # The 'with tokenizer.as_target_tokenizer():' block ensures the tokenizer handles the target language correctly\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "        \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the entire dataset\n",
    "tokenized_datasets = split_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Let's check the structure of our tokenized data\n",
    "print(tokenized_datasets['train'][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuning the Model\n",
    "\n",
    "Now we are ready to set up the training process. We start by loading the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the training arguments. These arguments control various hyperparameters like learning rate, batch size, number of epochs, and evaluation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "output_dir = f\"{model_name}-finetuned-en-to-fr\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3, # Increased epochs for better learning on small dataset\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(), # Use mixed precision if a GPU is available\n",
    "    push_to_hub=False # Set to True if you want to upload the model to the Hub\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a data collator. This will create batches of data and dynamically pad the texts to the length of the longest element in their batch. This is more efficient than padding all texts to a global maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the components ready, we can instantiate the `Seq2SeqTrainer` and start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    # We don't add a compute_metrics function here for simplicity,\n",
    "    # but for a real project, you would add one to calculate BLEU scores.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference with Pipeline\n",
    "\n",
    "After training is complete, the best model is saved in the output directory. We can now use this model for inference. The easiest way to do this is with the `pipeline` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trainer saves the best model in the 'output_dir' specified in TrainingArguments\n",
    "fine_tuned_model_path = f\"./{output_dir}/\"\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "translator = pipeline(\"translation_en_to_fr\", model=fine_tuned_model_path)\n",
    "\n",
    "# Let's test it with a sentence\n",
    "english_text = \"Hugging Face is a company based in New York City.\"\n",
    "french_translation = translator(english_text)\n",
    "\n",
    "print(f\"English: {english_text}\")\n",
    "print(f\"French: {french_translation[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "english_text_2 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "french_translation_2 = translator(english_text_2)\n",
    "\n",
    "print(f\"English: {english_text_2}\")\n",
    "print(f\"French: {french_translation_2[0]['translation_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
