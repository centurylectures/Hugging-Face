{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning BERT for Question Answering with Hugging Face ðŸ¤—\n",
        "\n",
        "This notebook provides a complete walkthrough for fine-tuning a BERT model on a question-answering task using the Hugging Face `transformers`, `datasets`, and `evaluate` libraries. We'll use the popular SQuAD (Stanford Question Answering Dataset) for this task.\n",
        "\n",
        "The process involves these key steps:\n",
        "1.  **Setup**: Install and import necessary libraries.\n",
        "2.  **Load Data**: Load the SQuAD dataset.\n",
        "3.  **Preprocessing**: Tokenize and prepare the text data for the model.\n",
        "4.  **Fine-Tuning**: Train the model using the `Trainer` API.\n",
        "5.  **Evaluation**: Evaluate the model's performance on the validation set.\n",
        "6.  **Inference**: Use the fine-tuned model for prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installations\n",
        "\n",
        "First, let's install the required libraries from Hugging Face. We need `datasets` to load our data, `transformers` for the model and tokenizer, and `evaluate` for the metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers[torch] datasets evaluate -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load the Dataset\n",
        "\n",
        "We'll use the SQuAD dataset, which is a standard benchmark for question answering. It consists of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the SQuAD dataset\n",
        "squad = load_dataset(\"squad\")\n",
        "\n",
        "print(squad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at a single example from the training set to understand its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(squad[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocessing the Data\n",
        "\n",
        "Preprocessing for question answering is more involved than for simple text classification. We need to:\n",
        "1.  Tokenize the **context** and the **question** together.\n",
        "2.  Handle long contexts that exceed the model's maximum sequence length (512 tokens for BERT).\n",
        "3.  Map the start and end positions of the answer in the original text to the tokenized input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# We'll use a distilled version of BERT for faster training, but you can use 'bert-base-uncased' for better performance\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handling Long Contexts\n",
        "\n",
        "When a context is too long, we can split it into several smaller chunks. The `stride` parameter creates an overlap between these chunks, ensuring that the answer span isn't cut in half. Each chunk will become a separate feature for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_length = 384  # The maximum length of a feature (question and context)\n",
        "doc_stride = 128  # The authorized overlap between two parts of the context when splitting it is needed.\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the questions and contexts, truncating only the context if the total length is too long.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # The 'return_overflowing_tokens' creates a mapping from a feature to its original example.\n",
        "    # We need this to map predictions back to their original context.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Now we need to label our data with the start and end token positions.\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Get the original example corresponding to this feature.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "        # If no answers are given, set the cls_index as the answer.\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start and end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Find the token start and end indices.\n",
        "            token_start_index = 0\n",
        "            while tokenized_examples.sequence_ids(i)[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while tokenized_examples.sequence_ids(i)[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # If the answer is not fully inside the current span, label it with (0, 0).\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise, find the exact start and end token positions.\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we apply this function to our entire dataset using `map`. This might take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)\n",
        "print(\"Preprocessing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Fine-Tuning the Model\n",
        "\n",
        "We're now ready to train our model. We'll use the `AutoModelForQuestionAnswering` class, which will load a pretrained BERT model with a question-answering head on top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define the `TrainingArguments`. This class holds all the hyperparameters for training, such as the learning rate, number of epochs, batch size, and where to save the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-squad\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False, # Set to True if you want to upload the model to the Hub\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we instantiate the `Trainer` and start the fine-tuning process. This will take a while, especially if you are not using a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_squad[\"train\"],\n",
        "    eval_dataset=tokenized_squad[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation\n",
        "\n",
        "After training, we need to evaluate our model's performance. The standard metrics for SQuAD are **Exact Match (EM)** and **F1-score**. This requires some post-processing to map the model's output (logits for start and end tokens) back to text spans in the original context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import collections\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size=20, max_answer_length=30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    \n",
        "    # Map features to their corresponding examples\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionary to store our predictions\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Loop over all the examples\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None \n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map back to the original context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            # Go through all possibilities for start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    if (start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char:end_char],\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "        \n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, get the raw predictions from the model on the validation set.\n",
        "raw_predictions = trainer.predict(tokenized_squad[\"validation\"])\n",
        "\n",
        "# The Trainer hides some columns, so we need to create a new dataset for post-processing.\n",
        "validation_features = squad[\"validation\"].map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=squad[\"validation\"].column_names\n",
        ")\n",
        "\n",
        "# Now, clean up the predictions.\n",
        "final_predictions = postprocess_qa_predictions(squad[\"validation\"], validation_features, raw_predictions.predictions)\n",
        "\n",
        "# Load the SQuAD metric from the `evaluate` library.\n",
        "metric = evaluate.load(\"squad\")\n",
        "\n",
        "# Format the predictions and labels for the metric.\n",
        "formatted_predictions = [{\n",
        "    \"id\": k,\n",
        "    \"prediction_text\": v\n",
        "} for k, v in final_predictions.items()]\n",
        "\n",
        "references = [{\n",
        "    \"id\": ex[\"id\"],\n",
        "    \"answers\": ex[\"answers\"]\n",
        "} for ex in squad[\"validation\"]]\n",
        "\n",
        "# Compute the metrics.\n",
        "results = metric.compute(predictions=formatted_predictions, references=references)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inference\n",
        "\n",
        "Now that we have a fine-tuned model, let's see how to use it to answer a new question. The easiest way is to use a `pipeline`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# You need to provide the path where the trainer saved the model.\n",
        "# This is typically inside the directory you specified in TrainingArguments.\n",
        "model_path = f\"{model_name}-finetuned-squad/checkpoint-XXXX\" # <-- IMPORTANT: Replace XXXX with the last checkpoint number\n",
        "\n",
        "# For this example, let's just use the model object we already have in memory\n",
        "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "context = \"\"\"\n",
        "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. \n",
        "It is named after the engineer Gustave Eiffel, whose company designed and built the tower. \n",
        "Constructed from 1887 to 1889 as the entrance to the 1889 World's Fair, it was initially criticized by some of France's leading artists and intellectuals for its design, \n",
        "but it has become a global cultural icon of France and one of the most recognizable structures in the world.\n",
        "\"\"\"\n",
        "\n",
        "question = \"Who designed the Eiffel Tower?\"\n",
        "\n",
        "result = qa_pipeline(question=question, context=context)\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {result['answer']}\")\n",
        "print(f\"Score: {result['score']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}