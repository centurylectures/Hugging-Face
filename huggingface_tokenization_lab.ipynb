{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e69d28f",
   "metadata": {},
   "source": [
    "\n",
    "# üî§ Hugging Face Tokenization Lab\n",
    "\n",
    "Welcome to the Hugging Face Tokenization lab! This lab will help you explore the core concepts of tokenization in NLP using the `transformers` and `tokenizers` libraries from Hugging Face.\n",
    "\n",
    "We will go from basic theory to complex customizations, including using pre-trained models and training your own tokenizer from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc1ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers tokenizers datasets --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1afd69",
   "metadata": {},
   "source": [
    "\n",
    "## üìò 1. What is Tokenization?\n",
    "\n",
    "Tokenization is the process of converting a text input (e.g., a sentence or paragraph) into a sequence of tokens that a model can understand.\n",
    "\n",
    "These tokens are usually integers or subwords representing the original text.\n",
    "\n",
    "Tokenization is the **first step** in most NLP pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07505a6",
   "metadata": {},
   "source": [
    "\n",
    "## üîç 2. Why Tokenization is Important\n",
    "\n",
    "Transformers cannot handle raw text directly. They require tokenized input.\n",
    "\n",
    "A good tokenizer ensures:\n",
    "\n",
    "- Efficient encoding of text\n",
    "- Handling of rare words\n",
    "- Preservation of meaning\n",
    "- Compatibility with model architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627ca9f5",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÇÔ∏è 3. Types of Tokenization\n",
    "\n",
    "1. **Character-level tokenization**\n",
    "2. **Word-level tokenization**\n",
    "3. **Subword tokenization** (most common for Transformers)\n",
    "\n",
    "Subword tokenizers break rare words into common components, e.g.:\n",
    "- `\"unhappiness\"` ‚Üí `[\"un\", \"happi\", \"ness\"]`\n",
    "\n",
    "Popular subword algorithms:\n",
    "- **BPE** (Byte-Pair Encoding)\n",
    "- **WordPiece**\n",
    "- **Unigram**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06977537",
   "metadata": {},
   "source": [
    "## ü§ñ 4. Using a Pretrained Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c76620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Tokenization is the first step in NLP!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d69c05",
   "metadata": {},
   "source": [
    "## üì• 5. Tokenizer Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc46c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode returns token IDs\n",
    "encoded = tokenizer.encode(text)\n",
    "print(encoded)\n",
    "\n",
    "# Decode\n",
    "print(tokenizer.decode(encoded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c8d58d",
   "metadata": {},
   "source": [
    "## üîê 6. Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf045fac",
   "metadata": {},
   "source": [
    "## üîì 7. Decoding Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decode one sequence\n",
    "tokenizer.decode(encoded)\n",
    "\n",
    "# Decode multiple sequences\n",
    "tokenizer.batch_decode([encoded])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eecb17",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 8. Tokenizer Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea80fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoded = tokenizer(text, add_special_tokens=True, padding=\"max_length\", truncation=True, max_length=12)\n",
    "encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff11912",
   "metadata": {},
   "source": [
    "## üß† 9. Attention Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa185b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(encoded['input_ids'])\n",
    "print(encoded['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ba99c",
   "metadata": {},
   "source": [
    "## üîÅ 10. Batch Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6906af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = tokenizer([\"Hello world!\", \"Tokenization is awesome.\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedd1a37",
   "metadata": {},
   "source": [
    "## üß™ 11. Training a Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Initialize\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(vocab_size=1000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "\n",
    "# Dummy training\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ag_news\", split=\"train[:1%]\")  # small subset\n",
    "texts = [x[\"text\"] for x in dataset]\n",
    "\n",
    "tokenizer.train_from_iterator(texts, trainer)\n",
    "tokenizer.save(\"custom-tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335db07",
   "metadata": {},
   "source": [
    "## üíæ 12. Using a Custom Tokenizer with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2075e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom-tokenizer.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\", sep_token=\"[SEP]\", mask_token=\"[MASK]\")\n",
    "\n",
    "fast_tokenizer(\"Let's test our custom tokenizer!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd2fdf",
   "metadata": {},
   "source": [
    "## üê¢ 13. Fast vs Slow Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb9c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.is_fast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227da87",
   "metadata": {},
   "source": [
    "\n",
    "## üîç 14. Subword Algorithms\n",
    "\n",
    "Hugging Face supports 3 main algorithms:\n",
    "\n",
    "- **BPE** (e.g., GPT-2)\n",
    "- **WordPiece** (e.g., BERT)\n",
    "- **Unigram** (e.g., XLNet)\n",
    "\n",
    "Each has different strategies for subword splitting and vocabulary learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50630908",
   "metadata": {},
   "source": [
    "## üß© 15. Pre-tokenizers and Normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d1c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents, Sequence\n",
    "\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62148d3e",
   "metadata": {},
   "source": [
    "## üßπ 16. Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e6287",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b0f83",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Conclusion\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "- What tokenization is and why it matters\n",
    "- How to use Hugging Face's pretrained tokenizers\n",
    "- How to decode, pad, truncate, and batch encode\n",
    "- How to train a custom tokenizer from scratch\n",
    "- How to apply normalizers, pre-tokenizers, and post-processors\n",
    "\n",
    "Now you're ready to build NLP pipelines with precise control over tokenization.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}