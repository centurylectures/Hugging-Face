{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184a2345",
   "metadata": {},
   "source": [
    "# Hugging Face Datasets â€” Endâ€‘toâ€‘End Tutorial\n",
    "\n",
    "*A handsâ€‘on, topicâ€‘byâ€‘topic guide with explanations before each code block.*\n",
    "\n",
    "> **Last updated:** 2025-09-17 09:11 UTC\n",
    "> \n",
    "> **What youâ€™ll learn**\n",
    "> - Installing and setting up `datasets`\n",
    "> - Loading, inspecting, and transforming datasets\n",
    "> - Efficient preprocessing with `map`, filtering, and splitting\n",
    "> - Streaming large datasets\n",
    "> - Working with text, images, and audio\n",
    "> - Interoperability with pandas, NumPy, PyTorch, and TensorFlow\n",
    "> - Saving, exporting, and versioning data\n",
    "> - Pushing datasets to the Hub and writing Dataset Cards\n",
    "> - Performance tips, caching, and troubleshooting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d66ac9",
   "metadata": {},
   "source": [
    "## 0) Prerequisites & Environment\n",
    "\n",
    "This notebook is designed to be **read topâ€‘toâ€‘bottom**, with each topic introduced by a markdown explanation followed by runnable code.\n",
    "Youâ€™ll need:\n",
    "\n",
    "- Python 3.8+\n",
    "- Recent versions of:\n",
    "  - `datasets`\n",
    "  - `pandas` (optional, for tabular interop)\n",
    "  - `numpy`\n",
    "  - `torch` or `tensorflow` (optional, for model training examples)\n",
    "  - `transformers` (optional, for tokenization examples)\n",
    "\n",
    "> **Tip:** If youâ€™re running in a restricted environment (e.g., no internet), you can still read through the code cells. Execute them later in a connected runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e58af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Install or update libraries in your environment\n",
    "# If you're in an offline environment, skip this. Otherwise, uncomment to run.\n",
    "# %pip install -U datasets pandas numpy transformers torch torchvision torchaudio tensorflow pillow soundfile pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d285e0b",
   "metadata": {},
   "source": [
    "## 1) Quickstart: Load and Peek at a Dataset\n",
    "\n",
    "`datasets.load_dataset` fetches datasets from the Hugging Face Hub (or disk). It returns a **DatasetDict** (for multiple splits) or a **Dataset**.\n",
    "\n",
    "Below we load the classic IMDB reviews dataset and take a quick look.\n",
    "\n",
    "> **Key ideas**\n",
    "> - Use `load_dataset(\"<namespace>/<name>\")` or just `load_dataset(\"<name>\")`.\n",
    "> - Access splits like `dataset[\"train\"]` and `dataset[\"test\"]`.\n",
    "> - Use `.features`, `.column_names`, `.num_rows`, and slicing to inspect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c767bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a small text dataset (will download on first run and cache locally)\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "# Peek\n",
    "imdb, imdb[\"train\"][0], imdb[\"train\"].features, imdb[\"train\"].column_names, imdb[\"train\"].num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbf54ef",
   "metadata": {},
   "source": [
    "## 2) Inspecting Structure & Metadata\n",
    "\n",
    "Understanding a datasetâ€™s schema is crucial. `features` defines column types; `DatasetInfo` contains license, citation, description, and more.\n",
    "\n",
    "> **Pro tips**\n",
    "> - `dataset.info.description` often mirrors the Dataset Card.\n",
    "> - Use `dataset.unique(\"label\")` or `value_counts()` via pandas for quick EDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97251897",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = imdb[\"train\"].info\n",
    "print(\"Description (truncated):\\n\", (info.description or \"\")[:500], \"...\")\n",
    "print(\"\\nLicense:\", info.license)\n",
    "print(\"\\nFeatures:\", imdb[\"train\"].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4bf908",
   "metadata": {},
   "source": [
    "## 3) Basic Operations: Select, Filter, and Map\n",
    "\n",
    "`datasets` uses **Apache Arrow** under the hood for efficient, memoryâ€‘aware ops.\n",
    "\n",
    "- **`select`**: take rows by index (or slice) â€” great for subsampling.\n",
    "- **`filter`**: keep rows matching a predicate.\n",
    "- **`map`**: transform rows; supports batched processing and multiprocessing.\n",
    "\n",
    "> **Immutability:** Operations return **new** datasets; originals remain intact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead5e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train = imdb[\"train\"]\n",
    "\n",
    "# Select a subset\n",
    "small_train = train.select(range(2000))\n",
    "\n",
    "# Filter by length\n",
    "def is_long(example):\n",
    "    return len(example[\"text\"].split()) > 50\n",
    "\n",
    "long_reviews = small_train.filter(is_long)\n",
    "\n",
    "# Map: add a simple feature (word count)\n",
    "def add_word_count(example):\n",
    "    example[\"word_count\"] = len(example[\"text\"].split())\n",
    "    return example\n",
    "\n",
    "with_wc = long_reviews.map(add_word_count)\n",
    "\n",
    "with_wc[0], with_wc.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34972bc4",
   "metadata": {},
   "source": [
    "## 4) Train/Validation/Test Splits\n",
    "\n",
    "If a dataset doesnâ€™t provide a validation split, you can create one deterministically.\n",
    "\n",
    "> **Determinism:** Set a `seed` for reproducible splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c36918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Create a validation split from the training set\n",
    "split = small_train.train_test_split(test_size=0.2, seed=42)\n",
    "split = DatasetDict({\n",
    "    \"train\": split[\"train\"],\n",
    "    \"validation\": split[\"test\"],\n",
    "    \"test\": imdb[\"test\"].select(range(2000))  # smaller test for demos\n",
    "})\n",
    "\n",
    "{key: ds.num_rows for key, ds in split.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff287d",
   "metadata": {},
   "source": [
    "## 5) Text Preprocessing with ðŸ¤— Transformers Tokenizers\n",
    "\n",
    "Use `map` with a tokenizer for fast, batched tokenization. Keep columns compact to save memory.\n",
    "\n",
    "> **Tips**\n",
    "> - Use `batched=True` and set `num_proc` for parallelism (where supported).\n",
    "> - Use `remove_columns` to drop raw text once tokenized (if you donâ€™t need it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf71c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=False)\n",
    "\n",
    "tokenized = split.map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838a24e4",
   "metadata": {},
   "source": [
    "## 6) Building PyTorch Dataloaders (Optional)\n",
    "\n",
    "Create `DataLoader`s directly from a `Dataset` using a **data collator** to pad dynamically.\n",
    "\n",
    "> **Why dynamic padding?** Saves compute by padding only to the longest sequence in the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a93ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "train_loader = DataLoader(tokenized[\"train\"], batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "batch = next(iter(train_loader))\n",
    "{ k: v.shape for k, v in batch.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f92edd",
   "metadata": {},
   "source": [
    "## 7) Interoperability with pandas & NumPy\n",
    "\n",
    "Convert to pandas when you need rich tabular EDA or plotting. Convert back if needed.\n",
    "\n",
    "> **Note:** Conversions materialize data in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c94288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = with_wc.to_pandas()\n",
    "df.head(), df[\"word_count\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef86ddc",
   "metadata": {},
   "source": [
    "## 8) Saving, Loading, and Exporting\n",
    "\n",
    "Persist datasets to disk to avoid repeated downloads or heavy preprocessing.\n",
    "\n",
    "- `save_to_disk` / `load_from_disk` for Arrowâ€‘backed format\n",
    "- `to_parquet` / `from_parquet` and `to_csv` for interop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "with_wc.save_to_disk(\"/mnt/data/imdb-with-wc\")\n",
    "reloaded = load_from_disk(\"/mnt/data/imdb-with-wc\")\n",
    "\n",
    "# Export a small sample to parquet and CSV\n",
    "sample = reloaded.select(range(100))\n",
    "sample.to_parquet(\"/mnt/data/imdb-sample.parquet\")\n",
    "sample.to_csv(\"/mnt/data/imdb-sample.csv\")\n",
    "print(\"Saved to /mnt/data/ (parquet & csv).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af91c10",
   "metadata": {},
   "source": [
    "## 9) Streaming Large Datasets\n",
    "\n",
    "For huge datasets, avoid downloading everything: use **streaming** to iterate examples lazily.\n",
    "\n",
    "> **Caveats:** Not all operations are supported in streaming mode (e.g., random access).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0676605",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamed = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\n",
    "it = iter(streamed)\n",
    "for _ in range(2):\n",
    "    ex = next(it)\n",
    "    print({k: (str(v)[:80] + '...') for k, v in ex.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b3f1f0",
   "metadata": {},
   "source": [
    "## 10) Working with Image Datasets\n",
    "\n",
    "`datasets` supports image columns via **PIL** and can apply transforms with `set_transform` or inside `map`.\n",
    "\n",
    "> **Example:** Load CIFARâ€‘10 and normalize images lazily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899fd847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "cifar10 = load_dataset(\"cifar10\")\n",
    "\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std  = [0.2470, 0.2435, 0.2616]\n",
    "\n",
    "def normalize_images(batch):\n",
    "    imgs = np.stack([np.array(img) for img in batch[\"img\"]], axis=0).astype(\"float32\") / 255.0\n",
    "    imgs = (imgs - mean) / std\n",
    "    batch[\"img_array\"] = imgs\n",
    "    return batch\n",
    "\n",
    "cifar10_small = cifar10[\"train\"].select(range(64)).with_format(\"numpy\")\n",
    "cifar10_small = cifar10_small.map(normalize_images, batched=True, batch_size=32)\n",
    "cifar10_small[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98907db",
   "metadata": {},
   "source": [
    "## 11) Working with Audio Datasets\n",
    "\n",
    "Audio columns use **librosa**/**soundfile**. Resampling and feature extraction can be done with `map`.\n",
    "\n",
    "> **Example:** Load a speech dataset and resample to 16 kHz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d50f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = load_dataset(\"PolyAI/minds14\", \"en-US\")\n",
    "\n",
    "from datasets import Audio\n",
    "speech = speech.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "sample = speech[\"train\"][0][\"audio\"]\n",
    "sample[\"sampling_rate\"], type(sample[\"array\"]), sample[\"array\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab20ff6",
   "metadata": {},
   "source": [
    "## 12) Metrics with `evaluate` (Optional)\n",
    "\n",
    "Use ðŸ¤— `evaluate` for standard metrics (accuracy, F1, BLEU, WER, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf3315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U evaluate\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "preds = [0, 1, 1, 0]\n",
    "refs  = [0, 1, 0, 0]\n",
    "accuracy.compute(predictions=preds, references=refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bc233c",
   "metadata": {},
   "source": [
    "## 13) Versioning & Sharing: Push Datasets to the Hub\n",
    "\n",
    "You can push processed datasets to your (private or public) Hugging Face repo.\n",
    "\n",
    "> **Steps**\n",
    "> 1. Login with `huggingface-cli login` or `huggingface_hub.login()`\n",
    "> 2. Give your dataset a repo name like `username/my-imdb-processed`\n",
    "> 3. Call `push_to_hub()`\n",
    "\n",
    "> **Privacy:** Use private repos for sensitive data and include a clear license.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699440cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()  # Follow prompts or pass a token string\n",
    "\n",
    "# Reuse the 'reloaded' dataset from section 8\n",
    "# reloaded.push_to_hub(\"username/my-imdb-processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a343d102",
   "metadata": {},
   "source": [
    "## 14) Writing a Good Dataset Card\n",
    "\n",
    "A **Dataset Card** documents motivation, composition, collection process, preprocessing, intended uses, and limitations.\n",
    "Include:\n",
    "- **Overview:** summary, source, size, splits\n",
    "- **Licensing & Rights**\n",
    "- **Ethical Considerations & Biases**\n",
    "- **Usage:** tasks, benchmarks, metrics\n",
    "- **Caveats:** known issues, annotation quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_card_template = \"\"\"\n",
    "# Dataset Card for <your-dataset-name>\n",
    "\n",
    "## Dataset Summary\n",
    "<1â€“3 sentences>\n",
    "\n",
    "## Supported Tasks and Leaderboards\n",
    "- Text Classification\n",
    "\n",
    "## Languages\n",
    "English\n",
    "\n",
    "## Dataset Structure\n",
    "- Splits: train/validation/test\n",
    "- Features: text (string), label (class)\n",
    "\n",
    "## Data Instances\n",
    "```\n",
    "{\"text\": \"...\", \"label\": 0}\n",
    "```\n",
    "\n",
    "## Data Fields\n",
    "- text: string â€” the review text\n",
    "- label: int â€” 0 = negative, 1 = positive\n",
    "\n",
    "## Data Splits\n",
    "| Split | #Examples |\n",
    "|------:|----------:|\n",
    "| train | 20000     |\n",
    "| valid | 5000      |\n",
    "| test  | 5000      |\n",
    "\n",
    "## Licensing Information\n",
    "MIT\n",
    "\n",
    "## Citation\n",
    "```\n",
    "@inproceedings{...}\n",
    "```\n",
    "\n",
    "## Ethical Considerations\n",
    "- Potential biases in user reviews.\n",
    "\n",
    "## Limitations\n",
    "- Binary sentiment only.\n",
    "\"\"\"\n",
    "\n",
    "print(dataset_card_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff5d13",
   "metadata": {},
   "source": [
    "## 15) Caching, Memory, and Performance Tips\n",
    "\n",
    "- **Cache**: The first `load_dataset` download is cached under `~/.cache/huggingface/datasets`. Subsequent loads are instant.\n",
    "- **Arrow format**: Columnar inâ€‘memory format enables fast slicing and vectorized ops.\n",
    "- Use `batched=True` (and optionally `num_proc`) in `map` for speedups.\n",
    "- Use `with_format(\"torch\")` / `\"tensorflow\"` / `\"numpy\"` to avoid extra conversions in training loops.\n",
    "- Remove large raw columns after feature extraction to reduce memory.\n",
    "- For **very large** data, use **streaming** and write out processed shards to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15119aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: set a target framework format to avoid per-batch conversion\n",
    "tokenized_torch = tokenized.with_format(\"torch\")\n",
    "next(iter(DataLoader(tokenized_torch[\"train\"], batch_size=8, collate_fn=data_collator))).keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749d5d1",
   "metadata": {},
   "source": [
    "## 16) Troubleshooting & Common Errors\n",
    "\n",
    "- **`ConnectionError` / 403 / 404**: Check internet, dataset name, or private repo access.\n",
    "- **`ArrowInvalid`**: Often due to mixed types in a column â€” clean or cast columns.\n",
    "- **`Killed` / OOM**: Reduce batch sizes, remove columns, or stream data.\n",
    "- **`transformers` tokenizer slow**: Use fast tokenizers (default), batched mapping, and multiprocessing where supported.\n",
    "- **Windows path issues**: Use shorter cache paths or move cache via env var `HF_HOME`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc123307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick helper: inspect memory footprint of a dataset\n",
    "def table_size_mb(ds):\n",
    "    # Estimate: sum of column buffers (approximate)\n",
    "    return round(ds.data.nbytes / (1024**2), 2)\n",
    "\n",
    "size_mb = table_size_mb(tokenized[\"train\"])\n",
    "print(f\"Approx Arrow payload size: {size_mb} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373fa06c",
   "metadata": {},
   "source": [
    "## 17) Where to Go Next\n",
    "\n",
    "- Explore the [Hugging Face Hub](https://huggingface.co/datasets) for thousands of datasets.\n",
    "- Read the official docs for advanced features (streaming, sharding, pausing/resuming maps).\n",
    "- Try adapting this workflow to your **own** dataset: load from local files with `load_dataset(\"csv\" | \"json\" | \"parquet\", data_files=...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc566815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: load from local files\n",
    "# local = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"test\": \"test.csv\"})"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
