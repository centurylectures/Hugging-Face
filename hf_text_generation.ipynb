{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning GPT-2 for Joke Generation\n",
    "\n",
    "This notebook contains the complete code to fine-tune a pre-trained GPT-2 model to generate jokes. We will use the `transformers` and `datasets` libraries from Hugging Face.\n",
    "\n",
    "**Steps:**\n",
    "1.  **Setup**: Install and import the required libraries.\n",
    "2.  **Load Dataset**: Load a dataset of short jokes from the Hugging Face Hub.\n",
    "3.  **Preprocessing**: Tokenize the dataset and format it for training.\n",
    "4.  **Training**: Fine-tune the GPT-2 model on our joke dataset.\n",
    "5.  **Inference**: Use the fine-tuned model within a `pipeline` to generate new jokes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install the necessary libraries. We need `transformers` for the model, `datasets` to handle the data, and `torch` as the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch accelerate -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import everything we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "We'll use the `short-jokes-dataset` from the Hugging Face Hub. It's a simple dataset with one column containing jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"short-jokes-dataset\", split=\"train\")\n",
    "\n",
    "# Let's take a look at a few examples\n",
    "print(dataset)\n",
    "for i in range(3):\n",
    "    print(f\"Joke {i+1}: {dataset[i]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "Next, we need to prepare the data for the model. This involves:\n",
    "1.  Loading the GPT-2 tokenizer.\n",
    "2.  Setting a padding token to handle jokes of different lengths.\n",
    "3.  Creating a function to tokenize the text. We will wrap each joke with special tokens (`<|startoftext|>` and `<|endoftext|>`) to teach the model the structure of a complete joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 doesn't have a default pad token, so we'll set it to the end-of-speech token.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Format each joke with start and end tokens\n",
    "    formatted_jokes = [f\"<|startoftext|>{joke}<|endoftext|>\" for joke in examples[\"text\"]]\n",
    "    \n",
    "    # Tokenize the formatted jokes\n",
    "    tokenized_output = tokenizer(\n",
    "        formatted_jokes,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128 # You can adjust this based on joke length\n",
    "    )\n",
    "    \n",
    "    # The model expects 'labels' for language modeling, which are the same as input_ids\n",
    "    tokenized_output[\"labels\"] = tokenized_output[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized_output\n",
    "\n",
    "# Apply the tokenization to the entire dataset\n",
    "# We use batched=True for faster processing\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(\"\\nTokenized dataset sample:\")\n",
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "\n",
    "Now we are ready to fine-tune the model.\n",
    "1.  Load the pre-trained `GPT2LMHeadModel`.\n",
    "2.  Define `TrainingArguments` to configure the training process (e.g., number of epochs, learning rate, output directory).\n",
    "3.  Create a `Trainer` instance and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Resize token embeddings because we added a new pad token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define the output directory for our fine-tuned model\n",
    "output_dir = \"./gpt2-joker\"\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,  # For a quick demonstration. Increase to 3-5 for better results.\n",
    "    per_device_train_batch_size=8, # Adjust based on your GPU memory\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(), # Use mixed precision if a GPU is available\n",
    "    report_to=\"none\", # Can be set to \"wandb\" or \"tensorboard\"\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting the training process...\")\n",
    "trainer.train()\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# Save the final model and tokenizer\n",
    "print(f\"Saving model to {output_dir}\")\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference with Pipeline\n",
    "\n",
    "With our model fine-tuned and saved, we can now use it to generate jokes. The `pipeline` function from `transformers` makes this incredibly easy.\n",
    "\n",
    "We will load our saved model into a `text-generation` pipeline and use it to generate a few jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model using the pipeline\n",
    "joke_generator = pipeline(\"text-generation\", model=output_dir, tokenizer=output_dir)\n",
    "\n",
    "# The prompt should be the start-of-text token we used during training\n",
    "prompt = \"<|startoftext|>\"\n",
    "\n",
    "print(\"--- Generating Jokes ---\\n\")\n",
    "\n",
    "generated_jokes = joke_generator(\n",
    "    prompt,\n",
    "    max_length=80, # Max length of the generated joke\n",
    "    num_return_sequences=5, # Number of jokes to generate\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    "    pad_token_id=tokenizer.eos_token_id # Set pad token ID\n",
    ")\n",
    "\n",
    "for i, joke in enumerate(generated_jokes):\n",
    "    # Clean up the output by removing the prompt and end token\n",
    "    joke_text = joke['generated_text'].replace(prompt, \"\").replace(\"<|endoftext|>\", \"\").strip()\n",
    "    print(f\"Joke {i+1}: {joke_text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
